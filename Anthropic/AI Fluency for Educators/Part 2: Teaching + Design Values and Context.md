***For this exercise, I used CoPilot and entered the following prompt based on the exercise instructions.***

# Opening the conversation:
  * Explain that you're an educator who wants to build a reusable summary of their teaching approach and context
  * Let the AI know this document will help establish shared understanding for future collaborations
  * Indicate you'd like the AI to help you think through all relevant aspects of your teaching practice
  * You can ask the AI to interview you about your teaching, OR
  * You can just start sharing information and ask the AI what else would be helpful to know

---

# Engineered Prompt:

I am an instructional designer (which puts me in the "Educator" camp, even though my role is not to teach the content, but to build it and ensure it's aligned
with best practices for adult learning and instructional design). I would like to build a reusable summary of my approach using the ADDIE/4-MAT/Gagne frameworks
for designing and developing courses. I understand that agile ID such as the Successive Approximation Model (SAM) and "minimum viable instruction (MVI)" methodologies
are now becoming more and more popular, so perhaps I'll need a reusable summary just on that as well. These documents will help me establish shared understanding for 
future collaborations and will also allow me to "feed" them into other AI models to accomplish the same thing. I would like you to help me think through all relevant aspects 
of the traditional ADDIE/4-MAT/Gagne unified approach as well as the newer, more in-demand SAM/MVI approach. Feel free to ask me or interview me about my design approach or 
anything else you think would be helpful to know.

---

# First Turn Response to the Prompt

Absolutely — and I love how you’re framing this. What you’re really asking for are **two reusable “approach briefs”** you can share with stakeholders *and* reuse as **AI input artifacts**:

1.  a **“structured / classic”** approach that unifies **ADDIE + 4MAT + Gagné** into one coherent workflow, and
2.  a **“modern / agile”** approach that unifies **SAM + MVI** (and the practical realities of iterative development, SME bandwidth, and rapid validation).

Below I’ll do three things:

*   **(A) Give you two reusable “summary documents”** in a format that works well for collaborations and for feeding into AI.
*   **(B) Give you a unified “translation layer”** so you can show stakeholders how the models map to each other.
*   **(C) Interview you with a tight set of questions** to customize these to *your* voice, constraints, and design standards (without turning this into a 40-question intake).

If you answer just the interview questions at the end, I can revise both docs into polished, “ready-to-share” versions.

***

## A) Reusable Summary #1 — Unified Traditional Approach (ADDIE + 4MAT + Gagné)

### 1) Purpose (what this approach optimizes for)

This approach is best when you need:

*   **Alignment and traceability** (business goals → performance outcomes → assessments → content)
*   **Consistency and governance** (regulated environments, formal reviews, multiple reviewers)
*   **Instructional integrity** (strong scaffolding, practice, feedback, assessment validity)
*   **Predictable delivery** (clear phases, sign-offs, stable scope)

### 2) Guiding principles (the “how we think”)

*   **Performance-first:** We design for what learners must *do* on the job, not what they must *know*.
*   **Constructive alignment:** Objectives, activities, and assessments must match the same performance and cognitive level.
*   **Cognitive load management:** Reduce extraneous load, sequence complexity, and use job-realistic contexts.
*   **Adult learning:** Respect autonomy, relevance, experience, and immediate application.
*   **Measurable impact:** Build evaluation in from the start.

### 3) The unified workflow (ADDIE backbone; 4MAT and Gagné inside)

Think of **ADDIE as the project lifecycle**, while **4MAT + Gagné guide lesson architecture**.

#### Phase 1 — Analyze (ADDIE)

**Outputs**

*   Business goal + performance problem statement
*   Audience + context analysis (roles, prior knowledge, constraints, motivation)
*   Task analysis (what good performance looks like, common errors)
*   Success metrics (learning + performance + business)
*   Delivery constraints (time, tools, accessibility, localization)

**Typical decisions**

*   Is training the right solution? (vs. job aid / process change / comms)
*   What’s the “minimum required competence” and what’s “nice-to-have”?
*   What must be assessed vs. supported via resources?

#### Phase 2 — Design (ADDIE) using 4MAT + Gagné

Here’s the practical integration:

##### A. Course-level architecture (4MAT as experience flow)

4MAT helps you ensure the course answers four learner questions:

1.  **Why? (Meaning & relevance)**
    *   Business stakes, WIIFM, story/context, learner pain points
2.  **What? (Concepts & models)**
    *   Key ideas, mental models, rules, definitions
3.  **How? (Practice & application)**
    *   Guided practice, cases, simulations, feedback loops
4.  **What if? (Transfer & adaptation)**
    *   Variation, edge cases, reflection, on-the-job plan

You can treat these as **modules** or as a **repeatable pattern within every lesson**.

##### B. Lesson-level mechanics (Gagné as micro-sequence)

Within each module/lesson, Gagné provides the teaching events:

1.  Gain attention
2.  Inform learners of objectives
3.  Stimulate recall of prior knowledge
4.  Present content
5.  Provide guidance
6.  Elicit performance (practice)
7.  Provide feedback
8.  Assess performance
9.  Enhance retention and transfer

**Practical note:** You don’t need all nine events in a rigid way every time. You use them as a **checklist to ensure nothing critical is missing**, especially practice + feedback + transfer.

##### C. Assessment blueprinting (ties everything together)

**Outputs**

*   Objective → assessment method map (what is measured, how, and criteria)
*   Rubrics / answer keys / performance checklists
*   Practice strategy plan (guided → independent)

#### Phase 3 — Develop (ADDIE)

**Outputs**

*   Storyboards / scripts / prototypes
*   Media assets and interaction specs
*   Accessibility compliance artifacts (captions, alt text, keyboard nav)
*   SME review cycles with clear acceptance criteria

**Quality checks**

*   Instructional: alignment, load, practice sufficiency, feedback quality
*   Technical: SCORM/xAPI if needed, responsiveness, browser/device support
*   Content: SME accuracy, tone/voice, localization readiness

#### Phase 4 — Implement (ADDIE)

**Outputs**

*   Launch plan + comms
*   Facilitator guide / learner support / job aids
*   LMS setup and tracking plan
*   Pilot delivery (even for self-paced: controlled release)

#### Phase 5 — Evaluate (ADDIE; formative + summative)

**Outputs**

*   Formative evaluation findings (pilot, usability, learner confusion points)
*   Summative evaluation plan (behavior/performance metrics, business KPIs)
*   Iteration backlog (what changes in next release)

### 4) What stakeholders can expect from you (your “contract”)

*   A documented **rationale** for design decisions
*   Clear artifacts: performance outcomes, blueprint, storyboards/prototypes
*   A consistent review process and definition of “done”
*   A measurable evaluation plan

### 5) What you need from collaborators

*   SME availability windows + turnaround times
*   Access to real examples, tools, and “good vs. bad” work samples
*   Agreement on success metrics and required proficiency level
*   Decision-maker for scope and acceptance

***

## B) Reusable Summary #2 — Agile Approach (SAM + MVI)

### 1) Purpose (what this approach optimizes for)

This approach is best when you need:

*   **Speed to value** (ship something useful quickly)
*   **Iterative validation** (learn what works via prototypes/pilots)
*   **Evolving requirements** (product/process changes, shifting SME input)
*   **Continuous improvement** (release cycles, analytics-driven refinement)

### 2) Guiding principles

*   **Build the smallest thing that changes performance (MVI).**
*   **Prototype early to reduce risk:** validate understanding, flow, difficulty, and feasibility.
*   **Iterate with real feedback:** SMEs, learners, analytics, observed performance.
*   **Prioritize transfer:** job-realistic practice beats content coverage.
*   **Definition of Done includes adoption signals:** completion, confidence, and on-the-job use.

### 3) The SAM cycle (practical version)

SAM is usually described as **Evaluate → Design → Develop** in repeating loops, often with a **Savvy Start** up front.

#### Step 0 — Savvy Start (fast alignment workshop)

**Outputs**

*   Performance goal + success metrics
*   Top 3–5 critical tasks/errors to target
*   Draft learner journey (modules or scenarios)
*   Risk list (content volatility, tech constraints, SME availability)
*   Prototype plan (what we will test first)

#### Loop 1 — Prototype (low fidelity → high learning)

**Artifacts**

*   Clickable outline / sample screen set / storyboard slice
*   Sample scenario + feedback logic
*   Draft assessment item(s)

**Validation questions**

*   Does this reflect the real workflow?
*   Is the difficulty right?
*   Are we targeting the correct mistakes and decisions?
*   Is the “voice” and UX working?

#### Loop 2 — Build MVP / MVI (Minimum Viable Instruction)

MVI is your “first releasable version,” not your final course.

**What MVI typically includes**

*   Only the **must-do** tasks and decisions
*   Just enough explanation to support practice
*   A small number of **high-quality practice reps** with feedback
*   A measurable check for competence
*   A job aid or performance support component, if needed

**What MVI intentionally defers**

*   Nice-to-have depth
*   Extra media polish
*   Large content libraries
*   Edge cases that affect few learners (unless high risk)

#### Loop 3+ — Iterate toward V1 and beyond

Use data to prioritize improvements:

*   Learner friction (drop-off points, repeated wrong answers)
*   SME feedback (accuracy and coverage gaps)
*   Business impact signals (ticket reduction, quality metrics, time-to-competency)
*   Support requests and manager observations

### 4) Agile artifacts stakeholders understand (so they feel safe)

To keep agile from feeling “unstructured,” you give visible anchors:

*   **North Star:** performance outcomes + success metrics
*   **Backlog:** prioritized learning requirements and content tasks
*   **Definition of Done:** acceptance criteria (instructional + technical + content)
*   **Release plan:** MVI → V1 → V1.1 improvements
*   **Decision log:** why we built it this way

### 5) Governance without slowing down

*   Weekly 30-min review cycles (tight agenda, decisions captured)
*   SME “single throat to choke” for acceptance
*   Versioning: prototypes are expected to be rough; releases must meet QA

***

## C) “Translation Layer” — How the models map (great for shared understanding)

Use this to explain to stakeholders that you’re not “switching religions,” you’re choosing a workflow.

### 1) ADDIE vs SAM

*   **ADDIE** = phased lifecycle with defined deliverables and sign-offs
*   **SAM** = repeating mini-ADDIE cycles with prototypes and fast feedback

In practice:

*   SAM still does analysis/design/development — just in smaller loops with earlier validation.

### 2) 4MAT + Gagné work in both

*   **4MAT** answers *experience flow* (Why/What/How/What-if)
*   **Gagné** ensures *instructional completeness* (attention, practice, feedback, transfer)

In practice:

*   You can apply 4MAT + Gagné inside *either* ADDIE *or* SAM.

### 3) MVI as scope strategy (not a separate model)

*   MVI decides **how much** to build first to achieve performance change
*   It pairs naturally with SAM (iterative releases)

***

## D) Your “Reusable AI Input” (copy/paste prompt block)

Here’s a compact version you can feed into other models/tools to keep your standards consistent:

```text
Instructional Design Approach (Reusable Standard)

I design for job performance and adult learning. I align business goals → performance outcomes → assessments → learning activities → content. I manage cognitive load, prioritize authentic practice with feedback, and build transfer supports (job aids, reflection, action plans).

Traditional workflow (ADDIE): Analyze (performance + audience + constraints + success metrics), Design (blueprint, objectives, assessment map), Develop (storyboards, assets, QA), Implement (launch/pilot), Evaluate (formative + summative; iterate).

Learning experience structure: 4MAT to ensure relevance and application flow (Why/What/How/What-if). Gagné events as a checklist to ensure attention, guidance, practice, feedback, assessment, and transfer are included.

Agile workflow (SAM/MVI): Use a Savvy Start to align quickly, prototype early, build Minimum Viable Instruction (smallest instruction that changes performance), release, collect feedback/analytics, and iterate via a prioritized backlog. Definition of Done includes instructional alignment, usability/accessibility, and measurable competence checks.
```

***

# Interview (so I can tailor these to *your* real approach)

Reply with short bullets — even partial answers are fine. I’ll then rewrite both summaries in your voice and produce a “one-pager” + “two-pager” version of each.

## 1) Your operating context

1.  What do you build most often: **self-paced eLearning**, **VILT**, **ILT**, **blended**, **cohort-based**, **performance support**, or **all**?
2.  Typical audience: internal employees, customers, partners? Any common constraints (time, tech, language, compliance)?
3.  Your usual timeline bands (e.g., 2 weeks, 6 weeks, 3 months)?

## 2) Your design “non-negotiables”

4.  What are your **top 3 quality standards**? (Example: alignment, practice density, accessibility, measurement, narrative coherence, inclusivity)
5.  How do you define “effective practice” in your work (scenarios, branching, case studies, live roleplay, knowledge checks, projects)?
6.  How do you handle SMEs who want “all the content”?

## 3) Your preferred artifacts & collaboration style

7.  What artifacts do you like producing: design doc, blueprint, storyboard, prototype, script, facilitator guide, job aids?
8.  What does your ideal review cycle look like (cadence, who approves, what “done” means)?

## 4) Your measurement philosophy

9.  What outcomes do you typically measure (confidence, knowledge, on-the-job behavior, KPIs)?
10. Do you use xAPI/analytics/LMS data routinely, sometimes, or rarely?

## 5) Your agile comfort level

11. When you go agile, what do you prototype first: a scenario, an interaction model, an assessment, the UI pattern, or a “thin slice” module?
12. What’s your definition of **MVI** in your environment (time to first release, minimum practice count, minimum assessment threshold)?

***

## Quick next step (pick one)

If you want, tell me which deliverables you’d like at the end:

**Option A:** Two “one-pagers” (Traditional + Agile) for stakeholders  
**Option B:** Two “two-pagers” + a mapping diagram (text-based)  
**Option C:** A full “playbook” (5–8 pages) with templates, checklists, and sample language for SME alignment

Just answer the interview questions (or even only 1–6 to start), and I’ll shape this into polished, reusable documents with your tone and your real-world constraints.

---

# Refined Prompt (Answers to the Question posed by Copilot in the First turn)
This is great context — thank you. Based on what you shared, I’m going to do **three concrete things right now (no deferral)**:

1.  **Refactor the two reusable approach summaries** so they actually reflect *your real operating environment* (CPE-driven webinars, short timelines, SME slide-dependency, internal public accounting context).
2.  **Introduce a third, very practical “CPE‑constrained Agile” variant** that acknowledges the reality of 1–2 week turnarounds and Zoom-first delivery.
3.  **Give you clean, reusable artifacts** you can (a) share with SMEs/stakeholders and (b) feed directly into other AI systems.

I’ll also call out where your instincts are already *very mature* (especially around architecture, narrative coherence, and alignment), because that’s central to how these summaries should position you.

***

# PART 1 — Your Design Identity (Explicitly Stated)

Before the frameworks, this matters for shared understanding:

> **You are not a facilitator-first educator. You are an instructional architect.**  
> Your value is in structuring learning experiences that are coherent, aligned, defensible for CPE, and repeatable under tight constraints.

This framing will help collaborators understand *why* your documents look the way they do.

***

# REUSABLE SUMMARY #1

## “Structured Instructional Architecture”

### (ADDIE + 4MAT + Gagné — Adapted for CPE & VILT/ILT)

### When I use this approach

This is my default approach for:

*   **CPE-eligible webinars (1–2 hours)**
*   **Recurring VILT/ILT curricula** (audit, tax new hires)
*   **Any program that needs defensibility, alignment, and repeatability**
*   **Multi-year offerings that evolve through iteration**

This approach prioritizes **narrative coherence, objective alignment, and instructional defensibility** over novelty.

***

### Core principles (non‑negotiables)

*   **Narrative coherence:** Content flows as a story, not a slide dump.
*   **Objective alignment:** Every section ties back to explicit learning objectives.
*   **Adult relevance:** Content is anchored in real client work, real decisions, real risks.
*   **CPE defensibility:** Learning objectives, instructional time, and knowledge checks are intentional and documentable.
*   **Separation of “must know” vs “nice to know”:** Excess content is preserved as enrichment, not forced into live time.

***

### The Unified Model (how the frameworks work together)

Think of this as **three layers**, not three competing models.

***

## Layer 1 — ADDIE as the Lifecycle (Project Architecture)

### Analyze (compressed, but intentional)

Typical outputs (even on a 1–2 week timeline):

*   Target audience + role context (e.g., audit seniors, new tax associates)
*   Business context (why this topic matters *now*)
*   Learning objectives written at the **appropriate cognitive level**
*   CPE constraints (duration, interaction requirements, assessment expectations)

> In practice: this may be a short design doc or even a structured email — but it always exists.

***

### Design (where most of your value lives)

This is where **4MAT and Gagné come in**.

***

## Layer 2 — 4MAT as Narrative Flow (Macro Structure)

You implicitly do this already; this just makes it explicit and reusable.

For a **1–2 hour webinar**, the structure usually looks like:

1.  **WHY (Relevance & Stakes)**
    *   Why this topic matters to the firm, clients, regulators
    *   Common mistakes or risks
    *   Framing the “problem space”

2.  **WHAT (Concepts & Rules)**
    *   Standards, guidance, frameworks, rules
    *   Mental models (how to think about the issue)

3.  **HOW (Application & Practice)**
    *   Scenario-based polling questions
    *   Decision points (“What would you do next?”)
    *   Mini case walkthroughs

4.  **WHAT IF (Transfer & Judgment)**
    *   Variations, edge cases, professional judgment
    *   “If this changes, how does your approach change?”

> This structure supports both **CPE requirements** and **adult learning**, even when interactivity is limited.

***

## Layer 3 — Gagné as a Quality Checklist (Micro Structure)

You don’t rigidly “run Gagné”; you **audit your design against it**.

For each major section, you ensure:

*   Attention is gained (story, risk, real example)
*   Objectives are clear (explicit or implicit)
*   Prior knowledge is activated (polls, rhetorical questions)
*   Content is presented clearly
*   Guidance is provided (examples, worked reasoning)
*   Performance is elicited (polls, scenarios)
*   Feedback is provided (why an answer is right/wrong)
*   Retention & transfer are reinforced (summary, takeaway, job aid)

***

### Develop (Slides + Supporting Materials)

*   Slides are **instructional artifacts**, not scripts
*   Polls are written as **decision checks**, not trivia
*   SME “extra content” is moved to:
    *   Appendix slides
    *   Participant handouts
    *   Optional enrichment resources

***

### Implement

*   Delivery may be facilitated by SMEs, but the **instructional spine is yours**
*   For recurring programs, you maintain:
    *   Version history
    *   Rationale for changes
    *   Consistent narrative across years

***

### Evaluate (Reality-Aware)

Current state:

*   Level 1 (smile sheets)
*   Level 2 (knowledge checks)
*   60‑day knowledge transfer pulse

Emerging direction:

*   Establish business baselines where feasible
*   Track deltas over time (even if imperfect)
*   Design with future xAPI readiness in mind

***

# REUSABLE SUMMARY #2

## “Agile Instructional Design”

### (SAM + MVI — Reality‑Checked for Your Environment)

This is **not startup agile**. This is **professional-services agile under CPE constraints**.

***

### When I use this approach

*   **1–2 week timelines**
*   Rapid-response webinars
*   Volatile content (standards updates, regulatory changes)
*   Limited SME availability
*   Situations where “perfect” is not possible, but **coherent and aligned** still is

***

### Agile principles (adapted)

*   **Time to first viable release matters**
*   **Prototype thinking even when deliverables are slides**
*   **Small, validated improvements beat big, late rewrites**
*   **Minimum Viable Instruction (MVI) is a scope decision, not a quality compromise**

***

## SAM in Practice (What It Actually Looks Like)

### Step 1 — Savvy Start (Often 30–60 minutes)

Outputs:

*   Finalized learning objectives
*   Agreed‑upon narrative arc
*   Decision on *what will be interactive*
*   Agreement on what will be deferred to appendix/enrichment

This may happen via:

*   A short working session
*   A structured intake document
*   A marked-up outline

***

### Step 2 — Prototype (Yes, Even for Slides)

Your “prototype” might be:

*   A single representative section
*   One scenario-based polling question with feedback
*   A draft agenda with timing

The goal:

*   Validate **level**, **tone**, and **expectations** early

***

### Step 3 — Build MVI (Minimum Viable Instruction)

In your context, **MVI usually means**:

*   The minimum content required to meet objectives and CPE requirements
*   A coherent narrative arc
*   At least some meaningful learner decision-making
*   Clear takeaways for on-the-job application

MVI does **not** mean:

*   No design
*   No practice
*   No standards

It means **intentional constraint**.

***

### Step 4 — Iterate (Within the Same Delivery or Next One)

Iteration may happen:

*   Between alpha → beta → gold
*   Between sessions of the same webinar
*   Year-over-year for recurring curricula

***

## What Agile Looks Like *Here* (Truthfully)

> “Agile” often means: *How fast can SMEs give me slides?*

Your value-add is:

*   Turning SME content into **instruction**
*   Preserving narrative and alignment under pressure
*   Making small, high-leverage improvements (better polls, clearer flow, sharper examples)

***

# REUSABLE SUMMARY #3

## “CPE‑Constrained Agile (Practical Hybrid)”

This is the one that will probably resonate most with your stakeholders.

### Key idea

> We work iteratively **within fixed delivery constraints**, not around them.

### Design moves you already make (now formalized):

*   Lock objectives early
*   Agree on narrative spine before slides exist
*   Design polling questions *before* content is finalized
*   Use appendices to manage scope creep
*   Improve interactivity incrementally without risking CPE compliance

***

# YOUR “AI FEED” VERSION (Tuned to You)

You can copy/paste this verbatim:

```text
I am an instructional designer working primarily in a public accounting, CPE-driven environment. I design learning as an instructional architect, prioritizing narrative coherence and alignment to learning objectives. Most offerings are 1–2 hour webinars delivered via Zoom, supported by polling questions as knowledge checks, with occasional self-paced eLearning and recurring VILT/ILT curricula.

I use ADDIE as a lifecycle framework, with 4MAT to structure the learner experience (Why/What/How/What-if) and Gagné’s events as a quality checklist to ensure attention, practice, feedback, and transfer. Excess SME content is preserved as enrichment rather than forced into live instructional time.

For rapid timelines, I apply an agile/SAM-informed approach, focusing on Minimum Viable Instruction (MVI): the smallest coherent, defensible learning experience that meets objectives and CPE requirements. MVI in my context is primarily driven by time to first release, without sacrificing narrative or alignment.

I prefer producing design documents, blueprints, and workflows, and I use alpha/beta/gold review cycles. Measurement is currently focused on knowledge, with growing emphasis on business outcomes as the learning ecosystem matures.
```

***

## Where you’re already ahead (and should own it)

*   Your **architectural mindset** is a strength, not a limitation.
*   Your handling of SME content creep is **textbook professional**.
*   Your discomfort with facilitation actually reinforces your identity as a designer, not a presenter.
*   Your instinct to push beyond Kirkpatrick Level 2 is exactly right — even if the ecosystem isn’t ready *yet*.

***

### If you want to go further (optional next steps)

I can:

*   Turn these into **1‑page PDFs** for stakeholders
*   Create a **“Webinar Design Playbook”** specifically for CPE environments
*   Help you design a **lightweight scenario library** that doesn’t threaten CPE credit
*   Draft language you can use with leadership to justify **MVI without fear**

If you want, tell me which of those would be most useful — or if you want these rewritten to sound more formal, more conversational, or more “enterprise-polished.”

