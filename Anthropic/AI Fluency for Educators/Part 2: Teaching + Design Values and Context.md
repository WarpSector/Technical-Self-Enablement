***For this exercise, I used CoPilot and entered the following prompt based on the exercise instructions.***

# Opening the conversation:
  * Explain that you're an educator who wants to build a reusable summary of their teaching approach and context
  * Let the AI know this document will help establish shared understanding for future collaborations
  * Indicate you'd like the AI to help you think through all relevant aspects of your teaching practice
  * You can ask the AI to interview you about your teaching, OR
  * You can just start sharing information and ask the AI what else would be helpful to know

---

# Engineered Prompt:

I am an instructional designer (which puts me in the "Educator" camp, even though my role is not to teach the content, but to build it and ensure it's aligned
with best practices for adult learning and instructional design). I would like to build a reusable summary of my approach using the ADDIE/4-MAT/Gagne frameworks
for designing and developing courses. I understand that agile ID such as the Successive Approximation Model (SAM) and "minimum viable instruction (MVI)" methodologies
are now becoming more and more popular, so perhaps I'll need a reusable summary just on that as well. These documents will help me establish shared understanding for 
future collaborations and will also allow me to "feed" them into other AI models to accomplish the same thing. I would like you to help me think through all relevant aspects 
of the traditional ADDIE/4-MAT/Gagne unified approach as well as the newer, more in-demand SAM/MVI approach. Feel free to ask me or interview me about my design approach or 
anything else you think would be helpful to know.

---

# First Turn Response to the Prompt

Absolutely — and I love how you’re framing this. What you’re really asking for are **two reusable “approach briefs”** you can share with stakeholders *and* reuse as **AI input artifacts**:

1.  a **“structured / classic”** approach that unifies **ADDIE + 4MAT + Gagné** into one coherent workflow, and
2.  a **“modern / agile”** approach that unifies **SAM + MVI** (and the practical realities of iterative development, SME bandwidth, and rapid validation).

Below I’ll do three things:

*   **(A) Give you two reusable “summary documents”** in a format that works well for collaborations and for feeding into AI.
*   **(B) Give you a unified “translation layer”** so you can show stakeholders how the models map to each other.
*   **(C) Interview you with a tight set of questions** to customize these to *your* voice, constraints, and design standards (without turning this into a 40-question intake).

If you answer just the interview questions at the end, I can revise both docs into polished, “ready-to-share” versions.

***

## A) Reusable Summary #1 — Unified Traditional Approach (ADDIE + 4MAT + Gagné)

### 1) Purpose (what this approach optimizes for)

This approach is best when you need:

*   **Alignment and traceability** (business goals → performance outcomes → assessments → content)
*   **Consistency and governance** (regulated environments, formal reviews, multiple reviewers)
*   **Instructional integrity** (strong scaffolding, practice, feedback, assessment validity)
*   **Predictable delivery** (clear phases, sign-offs, stable scope)

### 2) Guiding principles (the “how we think”)

*   **Performance-first:** We design for what learners must *do* on the job, not what they must *know*.
*   **Constructive alignment:** Objectives, activities, and assessments must match the same performance and cognitive level.
*   **Cognitive load management:** Reduce extraneous load, sequence complexity, and use job-realistic contexts.
*   **Adult learning:** Respect autonomy, relevance, experience, and immediate application.
*   **Measurable impact:** Build evaluation in from the start.

### 3) The unified workflow (ADDIE backbone; 4MAT and Gagné inside)

Think of **ADDIE as the project lifecycle**, while **4MAT + Gagné guide lesson architecture**.

#### Phase 1 — Analyze (ADDIE)

**Outputs**

*   Business goal + performance problem statement
*   Audience + context analysis (roles, prior knowledge, constraints, motivation)
*   Task analysis (what good performance looks like, common errors)
*   Success metrics (learning + performance + business)
*   Delivery constraints (time, tools, accessibility, localization)

**Typical decisions**

*   Is training the right solution? (vs. job aid / process change / comms)
*   What’s the “minimum required competence” and what’s “nice-to-have”?
*   What must be assessed vs. supported via resources?

#### Phase 2 — Design (ADDIE) using 4MAT + Gagné

Here’s the practical integration:

##### A. Course-level architecture (4MAT as experience flow)

4MAT helps you ensure the course answers four learner questions:

1.  **Why? (Meaning & relevance)**
    *   Business stakes, WIIFM, story/context, learner pain points
2.  **What? (Concepts & models)**
    *   Key ideas, mental models, rules, definitions
3.  **How? (Practice & application)**
    *   Guided practice, cases, simulations, feedback loops
4.  **What if? (Transfer & adaptation)**
    *   Variation, edge cases, reflection, on-the-job plan

You can treat these as **modules** or as a **repeatable pattern within every lesson**.

##### B. Lesson-level mechanics (Gagné as micro-sequence)

Within each module/lesson, Gagné provides the teaching events:

1.  Gain attention
2.  Inform learners of objectives
3.  Stimulate recall of prior knowledge
4.  Present content
5.  Provide guidance
6.  Elicit performance (practice)
7.  Provide feedback
8.  Assess performance
9.  Enhance retention and transfer

**Practical note:** You don’t need all nine events in a rigid way every time. You use them as a **checklist to ensure nothing critical is missing**, especially practice + feedback + transfer.

##### C. Assessment blueprinting (ties everything together)

**Outputs**

*   Objective → assessment method map (what is measured, how, and criteria)
*   Rubrics / answer keys / performance checklists
*   Practice strategy plan (guided → independent)

#### Phase 3 — Develop (ADDIE)

**Outputs**

*   Storyboards / scripts / prototypes
*   Media assets and interaction specs
*   Accessibility compliance artifacts (captions, alt text, keyboard nav)
*   SME review cycles with clear acceptance criteria

**Quality checks**

*   Instructional: alignment, load, practice sufficiency, feedback quality
*   Technical: SCORM/xAPI if needed, responsiveness, browser/device support
*   Content: SME accuracy, tone/voice, localization readiness

#### Phase 4 — Implement (ADDIE)

**Outputs**

*   Launch plan + comms
*   Facilitator guide / learner support / job aids
*   LMS setup and tracking plan
*   Pilot delivery (even for self-paced: controlled release)

#### Phase 5 — Evaluate (ADDIE; formative + summative)

**Outputs**

*   Formative evaluation findings (pilot, usability, learner confusion points)
*   Summative evaluation plan (behavior/performance metrics, business KPIs)
*   Iteration backlog (what changes in next release)

### 4) What stakeholders can expect from you (your “contract”)

*   A documented **rationale** for design decisions
*   Clear artifacts: performance outcomes, blueprint, storyboards/prototypes
*   A consistent review process and definition of “done”
*   A measurable evaluation plan

### 5) What you need from collaborators

*   SME availability windows + turnaround times
*   Access to real examples, tools, and “good vs. bad” work samples
*   Agreement on success metrics and required proficiency level
*   Decision-maker for scope and acceptance

***

## B) Reusable Summary #2 — Agile Approach (SAM + MVI)

### 1) Purpose (what this approach optimizes for)

This approach is best when you need:

*   **Speed to value** (ship something useful quickly)
*   **Iterative validation** (learn what works via prototypes/pilots)
*   **Evolving requirements** (product/process changes, shifting SME input)
*   **Continuous improvement** (release cycles, analytics-driven refinement)

### 2) Guiding principles

*   **Build the smallest thing that changes performance (MVI).**
*   **Prototype early to reduce risk:** validate understanding, flow, difficulty, and feasibility.
*   **Iterate with real feedback:** SMEs, learners, analytics, observed performance.
*   **Prioritize transfer:** job-realistic practice beats content coverage.
*   **Definition of Done includes adoption signals:** completion, confidence, and on-the-job use.

### 3) The SAM cycle (practical version)

SAM is usually described as **Evaluate → Design → Develop** in repeating loops, often with a **Savvy Start** up front.

#### Step 0 — Savvy Start (fast alignment workshop)

**Outputs**

*   Performance goal + success metrics
*   Top 3–5 critical tasks/errors to target
*   Draft learner journey (modules or scenarios)
*   Risk list (content volatility, tech constraints, SME availability)
*   Prototype plan (what we will test first)

#### Loop 1 — Prototype (low fidelity → high learning)

**Artifacts**

*   Clickable outline / sample screen set / storyboard slice
*   Sample scenario + feedback logic
*   Draft assessment item(s)

**Validation questions**

*   Does this reflect the real workflow?
*   Is the difficulty right?
*   Are we targeting the correct mistakes and decisions?
*   Is the “voice” and UX working?

#### Loop 2 — Build MVP / MVI (Minimum Viable Instruction)

MVI is your “first releasable version,” not your final course.

**What MVI typically includes**

*   Only the **must-do** tasks and decisions
*   Just enough explanation to support practice
*   A small number of **high-quality practice reps** with feedback
*   A measurable check for competence
*   A job aid or performance support component, if needed

**What MVI intentionally defers**

*   Nice-to-have depth
*   Extra media polish
*   Large content libraries
*   Edge cases that affect few learners (unless high risk)

#### Loop 3+ — Iterate toward V1 and beyond

Use data to prioritize improvements:

*   Learner friction (drop-off points, repeated wrong answers)
*   SME feedback (accuracy and coverage gaps)
*   Business impact signals (ticket reduction, quality metrics, time-to-competency)
*   Support requests and manager observations

### 4) Agile artifacts stakeholders understand (so they feel safe)

To keep agile from feeling “unstructured,” you give visible anchors:

*   **North Star:** performance outcomes + success metrics
*   **Backlog:** prioritized learning requirements and content tasks
*   **Definition of Done:** acceptance criteria (instructional + technical + content)
*   **Release plan:** MVI → V1 → V1.1 improvements
*   **Decision log:** why we built it this way

### 5) Governance without slowing down

*   Weekly 30-min review cycles (tight agenda, decisions captured)
*   SME “single throat to choke” for acceptance
*   Versioning: prototypes are expected to be rough; releases must meet QA

***

## C) “Translation Layer” — How the models map (great for shared understanding)

Use this to explain to stakeholders that you’re not “switching religions,” you’re choosing a workflow.

### 1) ADDIE vs SAM

*   **ADDIE** = phased lifecycle with defined deliverables and sign-offs
*   **SAM** = repeating mini-ADDIE cycles with prototypes and fast feedback

In practice:

*   SAM still does analysis/design/development — just in smaller loops with earlier validation.

### 2) 4MAT + Gagné work in both

*   **4MAT** answers *experience flow* (Why/What/How/What-if)
*   **Gagné** ensures *instructional completeness* (attention, practice, feedback, transfer)

In practice:

*   You can apply 4MAT + Gagné inside *either* ADDIE *or* SAM.

### 3) MVI as scope strategy (not a separate model)

*   MVI decides **how much** to build first to achieve performance change
*   It pairs naturally with SAM (iterative releases)

***

## D) Your “Reusable AI Input” (copy/paste prompt block)

Here’s a compact version you can feed into other models/tools to keep your standards consistent:

```text
Instructional Design Approach (Reusable Standard)

I design for job performance and adult learning. I align business goals → performance outcomes → assessments → learning activities → content. I manage cognitive load, prioritize authentic practice with feedback, and build transfer supports (job aids, reflection, action plans).

Traditional workflow (ADDIE): Analyze (performance + audience + constraints + success metrics), Design (blueprint, objectives, assessment map), Develop (storyboards, assets, QA), Implement (launch/pilot), Evaluate (formative + summative; iterate).

Learning experience structure: 4MAT to ensure relevance and application flow (Why/What/How/What-if). Gagné events as a checklist to ensure attention, guidance, practice, feedback, assessment, and transfer are included.

Agile workflow (SAM/MVI): Use a Savvy Start to align quickly, prototype early, build Minimum Viable Instruction (smallest instruction that changes performance), release, collect feedback/analytics, and iterate via a prioritized backlog. Definition of Done includes instructional alignment, usability/accessibility, and measurable competence checks.
```

***

# Interview (so I can tailor these to *your* real approach)

Reply with short bullets — even partial answers are fine. I’ll then rewrite both summaries in your voice and produce a “one-pager” + “two-pager” version of each.

## 1) Your operating context

1.  What do you build most often: **self-paced eLearning**, **VILT**, **ILT**, **blended**, **cohort-based**, **performance support**, or **all**?
2.  Typical audience: internal employees, customers, partners? Any common constraints (time, tech, language, compliance)?
3.  Your usual timeline bands (e.g., 2 weeks, 6 weeks, 3 months)?

## 2) Your design “non-negotiables”

4.  What are your **top 3 quality standards**? (Example: alignment, practice density, accessibility, measurement, narrative coherence, inclusivity)
5.  How do you define “effective practice” in your work (scenarios, branching, case studies, live roleplay, knowledge checks, projects)?
6.  How do you handle SMEs who want “all the content”?

## 3) Your preferred artifacts & collaboration style

7.  What artifacts do you like producing: design doc, blueprint, storyboard, prototype, script, facilitator guide, job aids?
8.  What does your ideal review cycle look like (cadence, who approves, what “done” means)?

## 4) Your measurement philosophy

9.  What outcomes do you typically measure (confidence, knowledge, on-the-job behavior, KPIs)?
10. Do you use xAPI/analytics/LMS data routinely, sometimes, or rarely?

## 5) Your agile comfort level

11. When you go agile, what do you prototype first: a scenario, an interaction model, an assessment, the UI pattern, or a “thin slice” module?
12. What’s your definition of **MVI** in your environment (time to first release, minimum practice count, minimum assessment threshold)?

***

## Quick next step (pick one)

If you want, tell me which deliverables you’d like at the end:

**Option A:** Two “one-pagers” (Traditional + Agile) for stakeholders  
**Option B:** Two “two-pagers” + a mapping diagram (text-based)  
**Option C:** A full “playbook” (5–8 pages) with templates, checklists, and sample language for SME alignment

Just answer the interview questions (or even only 1–6 to start), and I’ll shape this into polished, reusable documents with your tone and your real-world constraints.
